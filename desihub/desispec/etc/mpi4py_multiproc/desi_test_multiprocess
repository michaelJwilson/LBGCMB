#!/usr/bin/env python

from __future__ import absolute_import, division, print_function, unicode_literals, with_statement

from mpi4py import MPI

import sys
import os
import argparse
import numpy as np

import multiprocessing


# Global communicator

comm = MPI.COMM_WORLD
rank = comm.rank
nproc = comm.size

# Arguments

parser = argparse.ArgumentParser( description='Test 2-level MPI mixed with multiprocessing.' )
parser.add_argument( '--groupsize', required=False, type=int, default=1, help='size of MPI process group that operates on a single task.' )
parser.add_argument( '--ntask', required=False, type=int, default=10, help='the number of tasks distributed among MPI process groups.')
parser.add_argument('--tasksize', required=False, type=int, default=100, help='the number of small calculations in each task.  These are divided among the processes in each group, and then each MPI process uses multiprocessing to execute its local calculations.')
args = parser.parse_args()

groupsize = args.groupsize
if groupsize > nproc:
    groupsize = nproc

ntask = args.ntask
tasksize = args.tasksize


# Number of processes to use for multiprocessing

if 'SLURM_CPUS_PER_TASK' in os.environ.keys():
    mproc = int(os.environ['SLURM_CPUS_PER_TASK'])
else:
    mproc = max(1, multiprocessing.cpu_count() // 2)


# Fake work function that takes a seed, generates a 
# numpy random stream, and returns one element.

def compute(seed, element) :
    ret = None
    try :
        # set the seed
        np.random.seed(seed)
        # generate randoms up to the element we want
        data = np.random.random(size=(element+1))
        # extract the value we should return
        ret = data[element]
    except :
        ret = -1
    return ret


# The function for distribution of work in 
# the multiprocessing Pool.

def _func(arg) :
    return compute(**arg)


# Distribute some number of things among some number
# of workers

def dist_work(nwork, workers, id):
    ntask = 0
    firsttask = 0

    # if ID is out of range, ignore it
    if id < workers:
        if nwork < workers:
            if id < nwork:
                ntask = 1
                firsttask = id
        else:
            ntask = int(nwork / workers)
            leftover = nwork % workers
            if id < leftover:
                ntask += 1
                firsttask = id * ntask
            else:
                firsttask = ((ntask + 1) * leftover) + (ntask * (id - leftover))
    return (firsttask, ntask)


# Compute the number of groups and which group this process is in

ngroup = int(nproc / groupsize)
group = int(rank / groupsize)
group_rank = rank % groupsize

# This is the communicator within a group

comm_group = comm.Split(color=group, key=group_rank)

# This is the communicator among all processes with the same
# rank within their group.

comm_rank = comm.Split(color=group_rank, key=group)

# So now we have the processes divided into groups

# rank 00              : group 00, group_rank 00 
# rank 01              : group 00, group_rank 01
# ...
# rank <groupsize>     : group 01, group_rank 00
# rank <groupsize+1>   : group 01, group_rank 01
# ...
# rank <2*groupsize>   : group 02, group_rank 00
# rank <2*groupsize+1> : group 02, group_rank 01
# ...
# ranks >= (ngroup * groupsize) : group <ngroup>
# (these leftover ranks are assigned no tasks)


# Now statically divide the tasks among the process groups.
# Each group gets a contiguous range of tasks.

group_firsttask, group_ntask = dist_work(ntask, ngroup, group)

# Now that each group has its assigned tasks, we compute which
# "subtasks" every process in the group is going to run (using
# multiprocessing).

my_firstsubtask, my_nsubtask = dist_work(tasksize, groupsize, group_rank)

# Write out all this distribution info

for g in range(ngroup):
    if group == g:
        if group_rank == 0:
            print("Group {} of {} has {} processes".format(group+1, ngroup, comm_group.size))
            print("  assigned tasks {} - {}".format(group_firsttask, group_firsttask+group_ntask-1))
            sys.stdout.flush()
        for p in range(groupsize):
            if group_rank == p:
                print("  process {} running subtasks {} - {} with multiprocessing".format(p, my_firstsubtask, my_firstsubtask+my_nsubtask-1))
                sys.stdout.flush()
            comm_group.barrier()
    comm.barrier()

# The "results" from each task.  This is just the sum of randoms
# generated by the fake work function.

results = {}

if group_ntask > 0:
    # only do this if our group is assigned one or more tasks
    for t in range(group_firsttask, group_firsttask + group_ntask):
        # set up the multiprocessing args for this task
        mp_args = [ dict({'seed':t, 'element':x}) for x in range(tasksize) ]
        # create the pool
        pool = multiprocessing.Pool(mproc)
        # process only our assigned subtasks
        my_results = pool.map(_func, mp_args[my_firstsubtask:(my_firstsubtask+my_nsubtask)])
        # shut down the pool
        pool.close()
        pool.join()
        # take the sum of these local results
        my_task_results = np.sum(my_results)
        # reduce and distribute to the group
        results[t] = comm_group.allreduce(my_task_results, op=MPI.SUM) 
    comm_group.barrier()

# Here are some communication patterns to mimic what happens in
# the real pipeline

foo = group
if group_rank == 0:
    comm_rank.barrier()
bar = comm_group.bcast(foo, root=0)

comm.barrier()

# Write out the "results"

for g in range(ngroup):
    if group == g:
        if group_ntask > 0:
            if group_rank == 0:
                for t in range(group_firsttask, group_firsttask + group_ntask):
                    print("Task {:02} = {}".format(t, results[t]))
                    sys.stdout.flush()
    comm.barrier()

