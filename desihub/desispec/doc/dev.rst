.. _dev:


Pipeline Development and Internals
======================================

This section is for developers that would like more information on the pipeline internals and how to create new `workers` that can provide alternate processing for tasks in a given step of the pipeline.

Creating and Loading a Production
----------------------------------------

The high-level desi_pipe command (which is an entry point to desispec.scripts.pipe_prod.main) creates or updates a production.  After all DESI environment variables are set, the production directory hierarchy is created or updated by calling:

.. autodata:: desispec.pipeline.plan.create_prod
    :noindex:

Then the planning is done based on which raw data exists (see next section).  Finally, the slurm and shell scripts are created.  The number of processes needed for each task is obtained from the `worker` class for that task (see section below about workers).  At run time, the full production dependency graph (or a subset of the nights) is loaded with:

.. autodata:: desispec.pipeline.plan.load_prod
    :noindex:


Dependency Graph and Planning
----------------------------------

Whenever a production is created or updated, the raw data directory is scanned for available files.  Based on the raw data files, all `objects` produced by the pipeline steps can be determined ahead of time.  The interdependency of these objects across the steps of the pipeline are computed as a simple graph that is stored in a python dictionary.  This dictionary is written to disk as one yaml file per day.  The valid objects in a `plan` are:

.. autodata:: desispec.pipeline.common.graph_types
    :noindex:

The `night` object basically just serves as the parent object in the graph that all other daily objects descend from.  The `fibermap` and `pix` objects are the raw (pre-processed) data files.  The graph for a given day is created internally with the function:

.. autofunction:: desispec.pipeline.plan.graph_night
    :noindex:

A graph can be visualized by creating a `DOT` file and then using graphviz or other tools to draw a PNG file (for example):

.. autofunction:: desispec.pipeline.graph.graph_dot
    :noindex:

A graph can be `sliced` in various ways.  To get a list of nodes by name or by type, one can use:

.. autofunction:: desispec.pipeline.graph.graph_slice
    :noindex:

A graph may also be sliced by spectrograph with:

.. autofunction:: desispec.pipeline.graph.graph_slice_spec
    :noindex:

In addition to slicing a graph and keeping a piece of it, we can also `prune` the graph by removing a node:

.. autofunction:: desispec.pipeline.graph.graph_prune
    :noindex:

Every object in the graph has a `state`, and this state can be set recursively with:

.. autofunction:: desispec.pipeline.graph.graph_set_recursive
    :noindex:

During pipeline execution, different processes will accumulate information about the states of their assigned tasks.  The states across different processes can be merged with:

.. autofunction:: desispec.pipeline.graph.graph_merge
    :noindex:


Running Pipeline Steps
------------------------------

One or more pipeline steps can be run with the command line entry point `desi_pipe_run_mpi` (and the serial `desi_pipe_run`).  This command is called by the slurm and shell scripts generated by desi_pipe.  desi_pipe_run_mpi initializes MPI and passes its options and MPI.COMM_WORLD to this function:

.. autofunction:: desispec.pipeline.run.run_steps
    :noindex:

The number of processes used by each worker depends on the total number of processes and on the maximum processes supported by the selected Worker class for the given step (see section below).  As discussed in :ref:`pipeline`, failing tasks produce a special yaml file that can be used by desi_pipe_status to `retry` a task with a different communicator size and / or different input options.  When used in this mode, desi_pipe_status internally calls this function:

.. autofunction:: desispec.pipeline.run.retry_task
    :noindex:


Tracking State of Objects
-------------------------------

As groups of processes go through their assigned tasks (by calling the Worker's run method on each one), the state of each task is updated to one of these:

.. autodata:: desispec.pipeline.common.run_states
    :noindex:

At the end of each pipeline step, the current states of all tasks are merged and propogated across all processes with:

.. autofunction:: desispec.pipeline.graph.graph_merge
    :noindex:

This state information can be written to disk with:

.. autofunction:: desispec.pipeline.state.graph_db_write
    :noindex:

which is done at the end of each step of the pipeline.  The desi_pipe_status tool loads this state information for display purposes with:

.. autofunction:: desispec.pipeline.state.graph_db_read
    :noindex:

When desi_pipe_run_mpi starts, it always gets the full graph and checks which tasks are already done by calling:

.. autofunction:: desispec.pipeline.state.graph_db_check
    :noindex:


Serial Entry Points
-------------------------------

When developing new algorithms and debugging problems, it can be useful to run a pipeline task on a manually specified set of input files.  The desispec repo includes commandline tools which are just hooks into the underlying main function for each processing task.  This allows for convenient interactive testing of the underlying functions.  If a pipeline production is run with the log level set to "DEBUG", then the logs for each task will include the equivalent command that can be run to produce the output files.


Pipeline Task Workers
-------------------------------

The pipeline infrastructure in desispec is independent of the code which actually processes a single task.  All the information that the pipeline needs about the code which runs a task for a given step is encapsulated in the `Worker` class:

.. autoclass:: desispec.pipeline.task.Worker
    :members:
    :noindex:

There are default worker classes for all pipeline steps.  To use an alternate worker instance for one of the steps:

#.  Add a new Worker descendent class for your code.  (e.g. "WorkerMyFavorite")

#.  In the pipeline production directory, edit run/options.yaml and change the entry
    for the desired step to::

    <step>_worker: MyFavorite
    <step>_worker_opts: {}

    Adding any options needed to pass to the constructor.

#.  Re-run desi_pipe so that the slurm scripts can be updated to run at possibly 
    new concurrencies (if your worker supports a different max number of processes
    than the default worker).

